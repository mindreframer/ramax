defmodule PState.E2EMigrationTest do
  use ExUnit.Case, async: false

  alias PState
  alias PState.{Ref, Internal, MigrationWriter}
  alias PState.Adapters.{ETS, SQLite}
  alias Helpers.Value

  @moduledoc """
  Tests for RMX004_9A: End-to-End Migration Tests

  Comprehensive integration tests that verify the entire migration workflow:
  - Old data → read → migrate → background write → eventual consistency
  - Multiple migration patterns working together
  - Performance targets for production scenarios
  """

  # Comprehensive test schema with simple migrations that work with type checking
  defmodule TestSchema do
    use PState.Schema

    @moduledoc """
    Test schema with simple migrations for E2E testing.
    Uses type-checkable migrations (list → map, string → map).
    """

    entity :base_card do
      field(:id, :string)
      field(:front, :string)
      field(:back, :string)

      # Collection migration: list → map of Refs (type-checkable: list → map)
      field :translations, :map do
        migrate(fn
          ids when is_list(ids) ->
            Map.new(ids, fn id -> {id, Ref.new(:translation, id)} end)

          refs when is_map(refs) ->
            refs

          nil ->
            %{}
        end)
      end

      # String → Map migration (type-checkable)
      field :metadata, :map do
        migrate(fn
          map when is_map(map) and not is_struct(map) ->
            map

          str when is_binary(str) ->
            %{notes: str, created_at: nil}

          nil ->
            %{notes: "", created_at: nil}
        end)
      end

      # Simple ref field
      field(:deck, :ref)
    end

    entity :base_deck do
      field(:id, :string)
      field(:name, :string)

      # Collection migration
      field :cards, :map do
        migrate(fn
          ids when is_list(ids) ->
            Map.new(ids, fn id -> {id, Ref.new(:base_card, id)} end)

          refs when is_map(refs) ->
            refs

          nil ->
            %{}
        end)
      end
    end

    entity :translation do
      field(:id, :string)
      field(:language, :string)
      field(:text, :string)
    end
  end

  # Helper to create PState with schema
  defp create_pstate_with_schema(adapter \\ ETS, adapter_opts \\ []) do
    default_opts =
      case adapter do
        ETS -> [table_name: :"test_table_#{:erlang.unique_integer([:positive])}"]
        SQLite -> [path: ":memory:", table: "test_pstate_#{:erlang.unique_integer([:positive])}"]
      end

    opts = Keyword.merge(default_opts, adapter_opts)

    PState.new("root:test",
      adapter: adapter,
      adapter_opts: opts,
      schema: TestSchema
    )
  end

  # Helper to start MigrationWriter with test pstate
  defp start_migration_writer(pstate, opts \\ []) do
    flush_interval = Keyword.get(opts, :flush_interval, 100)
    batch_size = Keyword.get(opts, :batch_size, 100)

    # Stop if already running
    try do
      GenServer.stop(MigrationWriter)
    catch
      :exit, _ -> :ok
    end

    Process.sleep(10)

    {:ok, pid} =
      MigrationWriter.start_link(
        pstate: pstate,
        flush_interval: flush_interval,
        batch_size: batch_size
      )

    pid
  end

  # RMX004_9A_T1: Complete workflow test
  describe "RMX004_9A_T1: Complete workflow (old data → read → migrate → background write)" do
    test "old format data migrates on read and writes in background" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate, flush_interval: 100)

      # Store old format data directly
      old_card = %{
        id: "card1",
        front: "Hello",
        back: "Hola",
        # Old formats:
        translations: ["trans1", "trans2"],
        metadata: "old string notes"
      }

      pstate = put_in(pstate["base_card:card1"], old_card)

      # Clear cache to simulate fresh read
      pstate = %{pstate | cache: %{}}

      # Fetch - should trigger migration
      {:ok, migrated} = Internal.fetch_and_auto_migrate(pstate, "base_card:card1")

      # Verify migration occurred
      assert is_map(migrated.translations)
      assert migrated.translations["trans1"] == %Ref{key: "translation:trans1"}
      assert migrated.translations["trans2"] == %Ref{key: "translation:trans2"}
      assert migrated.metadata == %{notes: "old string notes", created_at: nil}

      # Wait for background write
      Process.sleep(200)

      # Fetch from adapter to verify background write
      {:ok, stored} = pstate.adapter.get(pstate.adapter_state, "base_card:card1")

      # Verify migrated format was written
      assert is_map(stored[:translations])
      assert stored[:metadata][:notes] == "old string notes"

      # Cleanup
      GenServer.stop(writer_pid)
    end
  end

  # RMX004_9A_T2: Eventual consistency test
  describe "RMX004_9A_T2: Eventual consistency (first read slow, second fast)" do
    test "first read triggers migration, second read uses migrated data" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate, flush_interval: 100)

      # Store old format
      old_card = %{
        id: "card1",
        front: "Test",
        translations: ["t1", "t2"],
        audio: %{url: %{primary: "http://test.mp3", format: "mp3"}},
        metadata: "notes"
      }

      pstate = put_in(pstate["base_card:card1"], old_card)
      pstate = %{pstate | cache: %{}}

      # First read - migration happens
      {time1_us, {:ok, result1}} =
        :timer.tc(fn -> Internal.fetch_and_auto_migrate(pstate, "base_card:card1") end)

      assert is_map(result1.translations)

      # Wait for background write to complete
      Process.sleep(200)

      # Clear cache to simulate fresh read from storage
      pstate = %{pstate | cache: %{}}

      # Second read - should use migrated data (no migration needed)
      {time2_us, {:ok, result2}} =
        :timer.tc(fn -> Internal.fetch_and_auto_migrate(pstate, "base_card:card1") end)

      # Both results should be identical (migrated format)
      assert result1.translations == result2.translations
      assert result1.audio == result2.audio
      assert result1.metadata == result2.metadata

      # Second read should generally be as fast or faster (though we can't guarantee it)
      # The key is that migration detection is fast
      time1_ms = time1_us / 1000
      time2_ms = time2_us / 1000

      # Both should be fast (<5ms)
      assert time1_ms < 5, "First read took #{time1_ms}ms, expected <5ms"
      assert time2_ms < 5, "Second read took #{time2_ms}ms, expected <5ms"

      GenServer.stop(writer_pid)
    end
  end

  # RMX004_9A_T3: Mixed format data test
  describe "RMX004_9A_T3: Mixed format data (some migrated, some not)" do
    test "handles entities in different migration states" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate)

      # Card 1: Old format (needs migration)
      old_card = %{
        id: "card1",
        front: "Old",
        translations: ["t1"],
        audio: %{url: %{primary: "http://old.mp3", format: "mp3"}},
        metadata: "old notes"
      }

      # Card 2: New format (already migrated)
      new_card = %{
        id: "card2",
        front: "New",
        translations: %{"t2" => %Ref{key: "translation:t2"}},
        audio: %{url: "http://new.mp3", format: "mp3"},
        metadata: %{notes: "new notes", created_at: nil}
      }

      # Card 3: Partially migrated
      partial_card = %{
        id: "card3",
        front: "Partial",
        translations: %{"t3" => %Ref{key: "translation:t3"}},
        audio: %{url: %{primary: "http://partial.mp3", format: "mp3"}},
        metadata: %{notes: "partial notes", created_at: nil}
      }

      pstate = put_in(pstate["base_card:card1"], old_card)
      pstate = put_in(pstate["base_card:card2"], new_card)
      pstate = put_in(pstate["base_card:card3"], partial_card)
      pstate = %{pstate | cache: %{}}

      # Fetch all cards
      {:ok, result1} = Internal.fetch_and_auto_migrate(pstate, "base_card:card1")
      {:ok, result2} = Internal.fetch_and_auto_migrate(pstate, "base_card:card2")
      {:ok, result3} = Internal.fetch_and_auto_migrate(pstate, "base_card:card3")

      # All should be in new format now
      assert is_map(result1.translations)
      assert is_map(result2.translations)
      assert is_map(result3.translations)

      assert result1.audio.url == "http://old.mp3"
      assert result2.audio.url == "http://new.mp3"
      assert result3.audio.url == "http://partial.mp3"

      assert result1.metadata.notes == "old notes"
      assert result2.metadata.notes == "new notes"
      assert result3.metadata.notes == "partial notes"

      GenServer.stop(writer_pid)
    end
  end

  # RMX004_9A_T4: Zero downtime schema change
  describe "RMX004_9A_T4: Zero downtime schema change" do
    test "application continues working during schema migration" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate)

      # Simulate active usage: mix of old and new data being read/written
      # during schema evolution

      # Write old format
      old_card = %{
        id: "card1",
        front: "Test",
        translations: ["t1"],
        audio: %{url: %{primary: "http://test.mp3", format: "mp3"}},
        metadata: "notes"
      }

      pstate = put_in(pstate["base_card:card1"], old_card)

      # Read (triggers migration)
      {:ok, migrated} = Internal.fetch_and_auto_migrate(pstate, "base_card:card1")
      assert is_map(migrated.translations)

      # Write new data in new format (application updated)
      new_card = %{
        id: "card2",
        front: "New",
        translations: %{"t2" => %Ref{key: "translation:t2"}},
        audio: %{url: "http://new.mp3", format: "mp3"},
        metadata: %{notes: "new", created_at: nil}
      }

      pstate = put_in(pstate["base_card:card2"], new_card)

      # Read old data again (should still work)
      pstate = %{pstate | cache: %{}}
      {:ok, still_works} = Internal.fetch_and_auto_migrate(pstate, "base_card:card1")
      assert is_map(still_works.translations)

      # Read new data (should work)
      {:ok, new_data} = Internal.fetch_and_auto_migrate(pstate, "base_card:card2")
      assert new_data.translations["t2"] == %Ref{key: "translation:t2"}

      # All operations successful - zero downtime!
      GenServer.stop(writer_pid)
    end
  end

  # RMX004_9A_T5: Helpers.Value integration
  describe "RMX004_9A_T5: Helpers.Value integration with migrations" do
    test "Value.get works with migrated data" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate)

      # Store old format
      old_card = %{
        id: "card1",
        front: "Hello",
        translations: ["t1"],
        audio: %{url: %{primary: "http://test.mp3", format: "mp3"}},
        metadata: "my notes"
      }

      pstate = put_in(pstate["base_card:card1"], old_card)
      pstate = %{pstate | cache: %{}}

      # Use Value.get to access nested data - should trigger migration
      front = Value.get(pstate, "base_card:card1.front")
      assert front == "Hello"

      # Access migrated nested field
      audio_url = Value.get(pstate, "base_card:card1.audio.url")
      # After migration, this should be a string, not a nested map
      assert audio_url == "http://test.mp3"

      # Access migrated metadata
      notes = Value.get(pstate, "base_card:card1.metadata.notes")
      assert notes == "my notes"

      GenServer.stop(writer_pid)
    end

    test "Value.insert works with migrated entities" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate)

      # Insert new card using Value.insert
      pstate = Value.insert(pstate, "base_card:card1.front", "Front text")
      pstate = Value.insert(pstate, "base_card:card1.back", "Back text")
      pstate = Value.insert(pstate, "base_card:card1.id", "card1")

      # Insert in new format
      pstate =
        Value.insert(pstate, "base_card:card1.metadata", %{notes: "inserted", created_at: nil})

      pstate = Value.insert(pstate, "base_card:card1.audio", %{url: "http://test.mp3", format: "mp3"})

      # Read back
      card = Value.get(pstate, "base_card:card1")
      assert card["front"] == "Front text"
      assert card["metadata"]["notes"] == "inserted"
      assert card["audio"]["url"] == "http://test.mp3"

      GenServer.stop(writer_pid)
    end
  end

  # RMX004_9A_T6: Per-track migration
  describe "RMX004_9A_T6: Per-track migration (1500 cards)" do
    test "migrates 1500 cards efficiently" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate, batch_size: 100, flush_interval: 1000)

      # Create 1500 cards in old format
      cards =
        Enum.map(1..1500, fn i ->
          {
            "base_card:card#{i}",
            %{
              id: "card#{i}",
              front: "Front #{i}",
              back: "Back #{i}",
              translations: ["t#{i}"],
              audio: %{url: %{primary: "http://audio#{i}.mp3", format: "mp3"}},
              metadata: "notes #{i}"
            }
          }
        end)

      # Store all cards
      pstate =
        Enum.reduce(cards, pstate, fn {key, card}, acc ->
          put_in(acc[key], card)
        end)

      # Clear cache
      pstate = %{pstate | cache: %{}}

      # Migrate all cards by reading them
      {duration_us, results} =
        :timer.tc(fn ->
          Enum.map(1..1500, fn i ->
            {:ok, migrated} = Internal.fetch_and_auto_migrate(pstate, "base_card:card#{i}")
            migrated
          end)
        end)

      duration_ms = duration_us / 1000

      # Verify all migrations succeeded
      assert length(results) == 1500

      Enum.each(results, fn card ->
        assert is_map(card.translations)
        assert is_map(card.audio)
        assert card.audio.url =~ "http://audio"
        assert is_map(card.metadata)
      end)

      # Should complete in reasonable time (<3000ms for 1500 cards = <2ms per card)
      assert duration_ms < 3000,
             "Migration of 1500 cards took #{duration_ms}ms, expected <3000ms"

      # Wait for background writes to complete
      Process.sleep(500)

      # Manually flush remaining
      MigrationWriter.flush()
      Process.sleep(100)

      # Sample check: verify some cards were written in migrated format
      {:ok, stored1} = pstate.adapter.get(pstate.adapter_state, "base_card:card1")
      {:ok, stored500} = pstate.adapter.get(pstate.adapter_state, "base_card:card500")
      {:ok, stored1500} = pstate.adapter.get(pstate.adapter_state, "base_card:card1500")

      assert is_map(stored1[:translations])
      assert is_map(stored500[:translations])
      assert is_map(stored1500[:translations])

      GenServer.stop(writer_pid)
    end
  end

  # RMX004_9A_T7: SQLite persistence
  describe "RMX004_9A_T7: SQLite persistence across restarts" do
    @tag :tmp_dir
    test "migrations persist across database restarts", %{tmp_dir: tmp_dir} do
      db_path = Path.join(tmp_dir, "test_migration.db")

      # First session: write old format
      pstate1 = create_pstate_with_schema(SQLite, path: db_path, table: "pstate_test")
      writer_pid1 = start_migration_writer(pstate1, flush_interval: 100)

      old_card = %{
        id: "card1",
        front: "Persistent",
        translations: ["t1"],
        audio: %{url: %{primary: "http://test.mp3", format: "mp3"}},
        metadata: "persistent notes"
      }

      pstate1 = put_in(pstate1["base_card:card1"], old_card)

      # Trigger migration by reading
      {:ok, _migrated} = Internal.fetch_and_auto_migrate(pstate1, "base_card:card1")

      # Wait for background write
      Process.sleep(200)
      MigrationWriter.flush()
      Process.sleep(100)

      # Close database
      GenServer.stop(writer_pid1)

      # Second session: reopen database
      pstate2 = create_pstate_with_schema(SQLite, path: db_path, table: "pstate_test")
      writer_pid2 = start_migration_writer(pstate2)

      # Read data - should already be migrated
      pstate2 = %{pstate2 | cache: %{}}
      {:ok, restored} = Internal.fetch_and_auto_migrate(pstate2, "base_card:card1")

      # Verify migrated format persisted
      assert is_map(restored.translations)
      assert restored.translations["t1"] == %Ref{key: "translation:t1"}
      assert restored.audio == %{url: "http://test.mp3", format: "mp3"}
      assert restored.metadata.notes == "persistent notes"

      GenServer.stop(writer_pid2)
    end
  end

  # RMX004_9A_T8: Combined migrations
  describe "RMX004_9A_T8: Collection + nested + rename migrations together" do
    test "all migration patterns work together in single entity" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate)

      # Entity with all old formats
      complex_card = %{
        id: "card1",
        front: "Complex",
        back: "Card",
        # Collection migration needed
        translations: ["t1", "t2", "t3"],
        # Nested migration needed
        audio: %{
          url: %{
            primary: "http://complex.mp3",
            format: "ogg"
          }
        },
        # Rename migration needed
        metadata: "complex notes"
      }

      pstate = put_in(pstate["base_card:card1"], complex_card)
      pstate = %{pstate | cache: %{}}

      # Migrate
      {:ok, migrated} = Internal.fetch_and_auto_migrate(pstate, "base_card:card1")

      # Verify all migrations applied
      # 1. Collection migration
      assert is_map(migrated.translations)
      assert migrated.translations["t1"] == %Ref{key: "translation:t1"}
      assert migrated.translations["t2"] == %Ref{key: "translation:t2"}
      assert migrated.translations["t3"] == %Ref{key: "translation:t3"}

      # 2. Nested migration
      assert migrated.audio == %{url: "http://complex.mp3", format: "ogg"}
      refute is_map(migrated.audio.url)

      # 3. Rename migration
      assert migrated.metadata == %{notes: "complex notes", created_at: nil}
      assert is_map(migrated.metadata)

      # Wait for background write
      Process.sleep(200)

      # Verify all migrations persisted
      {:ok, stored} = pstate.adapter.get(pstate.adapter_state, "base_card:card1")

      assert is_map(stored[:translations])
      assert stored[:audio][:url] == "http://complex.mp3"
      assert stored[:metadata][:notes] == "complex notes"

      GenServer.stop(writer_pid)
    end
  end

  # RMX004_9A_T9: Preloading with migrations
  describe "RMX004_9A_T9: Preloading with migrations" do
    test "preload works with entities that need migration" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate)

      # Create deck with old format cards
      old_card1 = %{
        id: "card1",
        front: "One",
        translations: ["t1"],
        audio: %{url: %{primary: "http://one.mp3", format: "mp3"}},
        metadata: "card one"
      }

      old_card2 = %{
        id: "card2",
        front: "Two",
        translations: ["t2"],
        audio: %{url: %{primary: "http://two.mp3", format: "mp3"}},
        metadata: "card two"
      }

      deck = %{
        id: "deck1",
        name: "Test Deck",
        cards: %{
          "card1" => %Ref{key: "base_card:card1"},
          "card2" => %Ref{key: "base_card:card2"}
        }
      }

      pstate = put_in(pstate["base_card:card1"], old_card1)
      pstate = put_in(pstate["base_card:card2"], old_card2)
      pstate = put_in(pstate["base_deck:deck1"], deck)
      pstate = %{pstate | cache: %{}}

      # Preload cards - should trigger migrations
      pstate = PState.preload(pstate, "base_deck:deck1", [:cards])

      # Verify cards are in cache and migrated
      assert Map.has_key?(pstate.cache, "base_card:card1")
      assert Map.has_key?(pstate.cache, "base_card:card2")

      card1 = pstate.cache["base_card:card1"]
      card2 = pstate.cache["base_card:card2"]

      assert is_map(card1.translations)
      assert is_map(card2.translations)
      assert card1.audio.url == "http://one.mp3"
      assert card2.audio.url == "http://two.mp3"

      GenServer.stop(writer_pid)
    end
  end

  # RMX004_9A_T10: Performance targets
  describe "RMX004_9A_T10: Performance targets (380k cards scenario)" do
    test "sample performance test with 1000 cards simulating production load" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate, batch_size: 100, flush_interval: 5000)

      # Create 1000 cards (scaled down from 380k for test speed)
      num_cards = 1000

      cards =
        Enum.map(1..num_cards, fn i ->
          {
            "base_card:card#{i}",
            %{
              id: "card#{i}",
              front: "Front #{i}",
              back: "Back #{i}",
              translations: ["t#{i}a", "t#{i}b"],
              audio: %{url: %{primary: "http://audio#{i}.mp3", format: "mp3"}},
              metadata: "notes for card #{i}"
            }
          }
        end)

      # Batch write old format cards
      pstate =
        Enum.reduce(cards, pstate, fn {key, card}, acc ->
          put_in(acc[key], card)
        end)

      # Performance test: First read (with migration)
      pstate = %{pstate | cache: %{}}

      {first_read_us, {:ok, first_card}} =
        :timer.tc(fn -> Internal.fetch_and_auto_migrate(pstate, "base_card:card500") end)

      first_read_ms = first_read_us / 1000

      assert is_map(first_card.translations)
      assert first_read_ms < 2, "First read took #{first_read_ms}ms, expected <2ms"

      # Wait for background write
      Process.sleep(300)

      # Performance test: Second read (no migration needed)
      pstate = %{pstate | cache: %{}}

      {second_read_us, {:ok, second_card}} =
        :timer.tc(fn -> Internal.fetch_and_auto_migrate(pstate, "base_card:card500") end)

      second_read_ms = second_read_us / 1000

      assert second_card == first_card
      assert second_read_ms < 0.5, "Second read took #{second_read_ms}ms, expected <0.5ms"

      # Performance test: Batch operations
      keys = Enum.map(1..100, fn i -> "base_card:card#{i}" end)

      {multi_get_us, {:ok, batch_results}} =
        :timer.tc(fn ->
          if function_exported?(pstate.adapter, :multi_get, 2) do
            pstate.adapter.multi_get(pstate.adapter_state, keys)
          else
            {:ok, %{}}
          end
        end)

      multi_get_ms = multi_get_us / 1000

      if map_size(batch_results) > 0 do
        assert multi_get_ms < 20, "multi_get (100 keys) took #{multi_get_ms}ms, expected <20ms"
      end

      # Performance test: Preloading
      deck = %{
        id: "deck1",
        name: "Large Deck",
        cards: Map.new(1..100, fn i -> {"card#{i}", %Ref{key: "base_card:card#{i}"}} end)
      }

      pstate = put_in(pstate["base_deck:deck1"], deck)
      pstate = %{pstate | cache: %{}}

      {preload_us, _preloaded_pstate} =
        :timer.tc(fn ->
          PState.preload(pstate, "base_deck:deck1", [:cards])
        end)

      preload_ms = preload_us / 1000

      assert preload_ms < 100,
             "Preload (100 refs) took #{preload_ms}ms, expected <100ms (relaxed for test)"

      GenServer.stop(writer_pid)
    end

    test "background flush performance" do
      pstate = create_pstate_with_schema()
      writer_pid = start_migration_writer(pstate, batch_size: 1000, flush_interval: 60000)

      # Queue 100 writes
      Enum.each(1..100, fn i ->
        MigrationWriter.queue_write("base_card:card#{i}", %{
          id: "card#{i}",
          front: "Test",
          translations: %{},
          audio: %{url: "http://test.mp3", format: "mp3"},
          metadata: %{notes: "", created_at: nil}
        })
      end)

      # Measure flush time
      {flush_us, :ok} = :timer.tc(fn -> MigrationWriter.flush() end)
      flush_ms = flush_us / 1000

      assert flush_ms < 50, "Flush (100 entries) took #{flush_ms}ms, expected <50ms"

      GenServer.stop(writer_pid)
    end
  end
end
